{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(output_file)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_sql(query, engine, chunksize\u001b[38;5;241m=\u001b[39mchunk_size):\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnappy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Read saved data for analysis\u001b[39;00m\n",
      "File \u001b[0;32m~/fhv/bachelor_thesis_infeo/.venv/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fhv/bachelor_thesis_infeo/.venv/lib/python3.10/site-packages/pandas/core/frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fhv/bachelor_thesis_infeo/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 480\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/fhv/bachelor_thesis_infeo/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:228\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    219\u001b[0m             table,\n\u001b[1;32m    220\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/fhv/bachelor_thesis_infeo/.venv/lib/python3.10/site-packages/pyarrow/parquet/core.py:1902\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **kwargs)\u001b[0m\n\u001b[1;32m   1900\u001b[0m use_int96 \u001b[38;5;241m=\u001b[39m use_deprecated_int96_timestamps\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1908\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1909\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_truncated_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_int96\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m            \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m   1928\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n\u001b[1;32m   1929\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/fhv/bachelor_thesis_infeo/.venv/lib/python3.10/site-packages/pyarrow/parquet/core.py:1021\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[0;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, write_page_checksum, sorting_columns, store_decimal_as_integer, **options)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collector \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata_collector\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1020\u001b[0m engine_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m \u001b[43m_parquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43msink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_deprecated_int96_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_byte_stream_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_engine_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_page_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_page_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compliant_nested_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_pagesize_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_page_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_page_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43msorting_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorting_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_decimal_as_integer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/fhv/bachelor_thesis_infeo/.venv/lib/python3.10/site-packages/pyarrow/_parquet.pyx:2151\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'append'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('../Credentials/awm_database_credentials.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "    \n",
    "HOST = data['host']\n",
    "PORT = data['port']\n",
    "USER = data['user']\n",
    "PASSWORD = data['password']\n",
    "DATABASE = data['database']\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
    "\n",
    "# estimate_query = \"\"\"\n",
    "# SELECT COUNT(*) AS total_rows\n",
    "# FROM waypoint wp\n",
    "# JOIN tracking t ON wp.id_tracking = t.id\n",
    "# WHERE t.duration > 300000\n",
    "# AND t.length > 500\n",
    "# AND t.is_exported = 0\n",
    "# AND (t.is_invalid IS NULL OR t.is_invalid = 0)\n",
    "# AND (SELECT MAX(latitude) - MIN(latitude) FROM waypoint WHERE id_tracking = t.id) > 0.001\n",
    "# AND (SELECT MAX(longitude) - MIN(longitude) FROM waypoint WHERE id_tracking = t.id) > 0.001;\n",
    "# \"\"\"\n",
    "\n",
    "# # Fetch row count estimate\n",
    "# row_count_df = pd.read_sql(estimate_query, engine)\n",
    "# print(f\"Estimated number of rows to load: {row_count_df['total_rows'][0]}\")\n",
    "\n",
    "# Query: Select only valid trackings and their waypoints in chunks\n",
    "query = \"\"\"\n",
    "SELECT wp.id_tracking, wp.id, wp.time, wp.type, wp.sequence, wp.comment, wp.speed, wp.heading, wp.duration, \n",
    "       wp.block_type, wp.log, wp.latitude, wp.longitude, wp.altitude, wp.meta_tag, wp.meta_value\n",
    "FROM waypoint wp\n",
    "JOIN tracking t ON wp.id_tracking = t.id\n",
    "WHERE t.duration > 300000\n",
    "AND t.length > 500\n",
    "AND t.is_exported = 0\n",
    "AND (t.is_invalid IS NULL OR t.is_invalid = 0)\n",
    "AND (SELECT MAX(latitude) - MIN(latitude) FROM waypoint WHERE id_tracking = t.id) > 0.001\n",
    "AND (SELECT MAX(longitude) - MIN(longitude) FROM waypoint WHERE id_tracking = t.id) > 0.001;\n",
    "\"\"\"\n",
    "\n",
    "chunk_size = 1000  # Adjust based on RAM capacity\n",
    "output_file = \"filtered_waypoints.parquet\"\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "for chunk in pd.read_sql(query, engine, chunksize=chunk_size):\n",
    "    chunk.to_parquet(output_file, engine=\"pyarrow\", compression=\"snappy\", index=False, append=False)\n",
    "    print(f\"Saved {len(chunk)} rows to {output_file}...\")\n",
    "\n",
    "df = pd.read_parquet(output_file)\n",
    "\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15183/4286415220.py:64: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  for chunk in pd.read_sql(query, conn, chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 1000000 rows...\n",
      "Processed 129967 rows...\n",
      "Data successfully saved to Parquet!\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq  \n",
    "import pyarrow as pa\n",
    "\n",
    "HOST = data['host']\n",
    "PORT = int(data['port']) \n",
    "USER = data['user']\n",
    "PASSWORD = data['password']\n",
    "DATABASE = data['database']\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    wp.id_tracking, wp.id, wp.time, wp.type, wp.sequence, wp.comment, \n",
    "    wp.speed, wp.heading, wp.duration, wp.block_type, wp.log, \n",
    "    wp.latitude, wp.longitude, wp.altitude, wp.meta_tag, wp.meta_value\n",
    "FROM waypoint wp\n",
    "JOIN tracking t ON wp.id_tracking = t.id\n",
    "WHERE \n",
    "    t.duration BETWEEN 18000000000 AND 360000000000  -- Between 0.5 Hour and 10 Hours\n",
    "    AND t.length BETWEEN 5 AND 150  -- Between 5km and 150km\n",
    "    AND (t.is_invalid IS NULL OR t.is_invalid = 0)\n",
    "    AND EXISTS (\n",
    "        SELECT 1 FROM waypoint w \n",
    "        WHERE w.id_tracking = t.id\n",
    "        HAVING COUNT(*) > 10  -- Ensure at least 10 waypoints exist\n",
    "    )\n",
    "    AND (\n",
    "        (SELECT MAX(latitude) FROM waypoint WHERE id_tracking = t.id) - \n",
    "        (SELECT MIN(latitude) FROM waypoint WHERE id_tracking = t.id)\n",
    "    ) > 0.0005  -- At least ~50m in latitude\n",
    "    AND (\n",
    "        (SELECT MAX(longitude) FROM waypoint WHERE id_tracking = t.id) - \n",
    "        (SELECT MIN(longitude) FROM waypoint WHERE id_tracking = t.id)\n",
    "    ) > 0.0005  -- At least ~50m in longitude\n",
    "ORDER BY t.duration DESC;  -- Sort by duration, longest trips first\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parquet_file = \"gps_data_relaxed_parameters_more.parquet\"\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=HOST,\n",
    "    port=PORT,\n",
    "    user=USER,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    cursorclass=pymysql.cursors.SSCursor  \n",
    ")\n",
    "\n",
    "chunk_size = 1000000 \n",
    "\n",
    "first_chunk = True  \n",
    "\n",
    "try:\n",
    "    for chunk in pd.read_sql(query, conn, chunksize=chunk_size):\n",
    "        table = pa.Table.from_pandas(chunk)\n",
    "\n",
    "        if first_chunk:\n",
    "            pq.write_table(table, parquet_file, compression=\"snappy\")\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            with pq.ParquetWriter(parquet_file, table.schema, compression=\"snappy\") as writer:\n",
    "                writer.write_table(table)\n",
    "\n",
    "        print(f\"Processed {len(chunk)} rows...\")\n",
    "    \n",
    "    print(\"data saved\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    conn.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8359/2681530646.py:48: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  for chunk in pd.read_sql(query, conn, chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 10000000 rows...\n",
      "Processed 7759733 rows...\n",
      "data saved\n"
     ]
    }
   ],
   "source": [
    "# no filter version. (Load all data)\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import os\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq  \n",
    "import pyarrow as pa\n",
    "\n",
    "with open('../Credentials/awm_database_credentials.json') as data_file:\n",
    "    data = json.load(data_file)\n",
    "\n",
    "HOST = data['host']\n",
    "PORT = int(data['port']) \n",
    "USER = data['user']\n",
    "PASSWORD = data['password']\n",
    "DATABASE = data['database']\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    wp.id_tracking, wp.id, wp.time, wp.type, wp.sequence, wp.comment, \n",
    "    wp.speed, wp.heading, wp.duration, wp.block_type, wp.log, \n",
    "    wp.latitude, wp.longitude, wp.altitude, wp.meta_tag, wp.meta_value\n",
    "FROM waypoint wp\n",
    "JOIN tracking t ON wp.id_tracking = t.id;\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parquet_file = \"all_gps_data_without_sorting.parquet\"\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=HOST,\n",
    "    port=PORT,\n",
    "    user=USER,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    cursorclass=pymysql.cursors.SSCursor  \n",
    ")\n",
    "\n",
    "chunk_size = 10000000 # was beofre 100.000 and working\n",
    "\n",
    "first_chunk = True  \n",
    "\n",
    "try:\n",
    "    for chunk in pd.read_sql(query, conn, chunksize=chunk_size):\n",
    "        table = pa.Table.from_pandas(chunk)\n",
    "\n",
    "        if first_chunk:\n",
    "            pq.write_table(table, parquet_file, compression=\"snappy\")\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            with pq.ParquetWriter(parquet_file, table.schema, compression=\"snappy\") as writer:\n",
    "                writer.write_table(table)\n",
    "\n",
    "        print(f\"Processed {len(chunk)} rows...\")\n",
    "    \n",
    "    print(\"data saved\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    conn.close() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
